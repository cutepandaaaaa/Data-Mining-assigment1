import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
from sklearn.model_selection import cross_val_score

# Step 1: Load the training dataset 'adult.data'
train_data = pd.read_csv('adult.data', header=None)

# Step 2: Add column names for the training dataset
train_data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',
                      'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',
                      'hours-per-week', 'native-country', 'income']

# Step 3: Clean column names by stripping leading/trailing spaces
train_data.columns = train_data.columns.str.strip()

# Step 4: Handle target variable: Convert income labels to binary (<=50K -> 0, >50K -> 1)
train_data['income'] = train_data['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)

# Step 5: Remove rows with missing values from the training dataset
train_data = train_data.dropna()

# Step 6: Split the training dataset into features (X_train) and target (y_train)
X_train = train_data.drop('income', axis=1)  # Features
y_train = train_data['income']  # Target

# Step 7: Load the validation dataset 'adult.test'
test_data = pd.read_csv('adult.test', header=None, skiprows=1)  # skiprows=1 to ignore the header

# Step 8: Add column names for the validation dataset
test_data.columns = train_data.columns  # Use the same column names as training data

# Step 9: Clean column names by stripping leading/trailing spaces
test_data.columns = test_data.columns.str.strip()

# Step 10: Handle target variable: Convert income labels to binary (<=50K -> 0, >50K -> 1)
test_data['income'] = test_data['income'].apply(lambda x: 1 if x.strip() == '>50K.' else 0)

# Step 11: Remove rows with missing values from the validation dataset
test_data = test_data.dropna()

# Step 12: Split the validation dataset into features (X_test) and target (y_test)
X_test = test_data.drop('income', axis=1)  # Features
y_test = test_data['income']  # Target

# Step 13: Identify categorical and numerical columns
categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 
                       'race', 'sex', 'native-country']
numerical_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']

# Step 14: Preprocessing - OneHotEncoding for categorical columns and scaling for numerical columns
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), numerical_columns),
    ('cat', OneHotEncoder(), categorical_columns)
])

# Step 15: Create pipelines for both models with preprocessing
log_reg_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

rf_clf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

# Step 16: Train the models

# Model 1: Logistic Regression
log_reg_pipeline.fit(X_train, y_train)

# Model 2: Random Forest Classifier
rf_clf_pipeline.fit(X_train, y_train)

# Step 17: Make predictions on the validation dataset
log_reg_preds = log_reg_pipeline.predict(X_test)
rf_clf_preds = rf_clf_pipeline.predict(X_test)

# Step 18: Evaluate the models

# 18.1: Accuracy
log_reg_acc = accuracy_score(y_test, log_reg_preds)
rf_clf_acc = accuracy_score(y_test, rf_clf_preds)
print(f"Logistic Regression Accuracy: {log_reg_acc}")
print(f"Random Forest Accuracy: {rf_clf_acc}")

# 18.2: Confusion Matrix
log_reg_cm = confusion_matrix(y_test, log_reg_preds)
rf_clf_cm = confusion_matrix(y_test, rf_clf_preds)
print("\nLogistic Regression Confusion Matrix:")
print(log_reg_cm)
print("\nRandom Forest Confusion Matrix:")
print(rf_clf_cm)

# 18.3: Classification Report (Precision, Recall, F1-Score)
log_reg_report = classification_report(y_test, log_reg_preds)
rf_clf_report = classification_report(y_test, rf_clf_preds)
print("\nLogistic Regression Classification Report:")
print(log_reg_report)
print("\nRandom Forest Classification Report:")
print(rf_clf_report)

# 18.4: ROC-AUC Score
log_reg_roc_auc = roc_auc_score(y_test, log_reg_pipeline.predict_proba(X_test)[:, 1])
rf_clf_roc_auc = roc_auc_score(y_test, rf_clf_pipeline.predict_proba(X_test)[:, 1])
print(f"\nLogistic Regression ROC-AUC Score: {log_reg_roc_auc}")
print(f"Random Forest ROC-AUC Score: {rf_clf_roc_auc}")

# 18.5: Cross-validation
log_reg_cv = cross_val_score(log_reg_pipeline, X_train, y_train, cv=5)
rf_clf_cv = cross_val_score(rf_clf_pipeline, X_train, y_train, cv=5)
print(f"\nLogistic Regression Cross-Validation Scores: {log_reg_cv}")
print(f"Random Forest Cross-Validation Scores: {rf_clf_cv}")
